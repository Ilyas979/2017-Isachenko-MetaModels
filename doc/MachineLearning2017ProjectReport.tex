\documentclass[a4paper,14pt]{article} 
% Этот шаблон документа разработан в 2014 году 
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% «Документы и презентации в \LaTeX», записанном НИУ ВШЭ 
% для Coursera.org: http://coursera.org/course/latex . 
% Исходная версия шаблона —- 
% https://www.writelatex.com/coursera/latex/5.3 

% В этом документе преамбула 

%%% Работа с русским языком 
\usepackage{cmap} % поиск в PDF 
\usepackage{mathtext} % русские буквы в формулах 
\usepackage[T2A]{fontenc} % кодировка 
\usepackage[cp1251]{inputenc} % кодировка исходного текста 
\usepackage[english]{babel} % локализация и переносы 
\usepackage{indentfirst} 
\frenchspacing
\usepackage[shortlabels]{enumitem}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}} 
\renewcommand{\phi}{\ensuremath{\varphi}} 
\renewcommand{\kappa}{\ensuremath{\varkappa}} 
\renewcommand{\le}{\ensuremath{\leqslant}} 
\renewcommand{\leq}{\ensuremath{\leqslant}} 
\renewcommand{\ge}{\ensuremath{\geqslant}} 
\renewcommand{\geq}{\ensuremath{\geqslant}} 
\renewcommand{\emptyset}{\varnothing} 
\newcommand{\T}{{\text{\footnotesize\sffamily\upshape\mdseries T}}}


%%% Дополнительная работа с математикой 
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools, bm} % AMS 
\usepackage{icomma} % "Умная" запятая: $0,2$ —- число, $0, 2$ —- перечисление 

%% Номера формул 
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте. 
%\usepackage{leqno} % Нумереация формул слева 

%% Свои команды 
\DeclareMathOperator{\sgn}{\mathop{sgn}}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Перенос знаков в формулах (по Львовскому) 
%\newcommand*{\hm}[1]{#1\nobreak\discretionary{} 
%	{\hbox{$\mathsurround=0pt #1$}}{}} 

%%% Работа с картинками 
\usepackage{graphicx} % Для вставки рисунков 
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка 
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{} 
\usepackage{wrapfig} % Обтекание рисунков текстом 
\usepackage{float}
\usepackage{subfig}

%%% Работа с таблицами 
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами 
\usepackage{longtable} % Длинные таблицы 
\usepackage{multirow} % Слияние строк в таблице 

%%% Теоремы 
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять. 
\newtheorem{theorem}{Теорема}[section] 
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение" 
\newtheorem{corollary}{Следствие}[theorem] 
\newtheorem{problem}{Задача}[section] 

\theoremstyle{remark} % "Примечание" 
\newtheorem*{nonum}{Решение} 

%%% Программирование 
\usepackage{etoolbox} % логические операторы

%%% Страница 
\usepackage{extsizes} % Возможность сделать 14-й шрифт 
\usepackage{geometry} % Простой способ задавать поля 
\geometry{top=20mm} 
\geometry{bottom=25mm} 
\geometry{left=20mm} 
\geometry{right=20mm} 
% 
%\usepackage{fancyhdr} % Колонтитулы 
% \pagestyle{fancy} 
%\renewcommand{\headrulewidth}{0pt} % Толщина линейки, отчеркивающей верхний колонтитул 
% \lfoot{Нижний левый} 
% \rfoot{Нижний правый} 
% \rhead{Верхний правый} 
% \chead{Верхний в центре} 
% \lhead{Верхний левый} 
% \cfoot{Нижний в центре} % По умолчанию здесь номер страницы 

\usepackage{setspace} % Интерлиньяж 
\onehalfspacing % Интерлиньяж 1.5 
%\doublespacing % Интерлиньяж 2 
%\singlespacing % Интерлиньяж 1
\setlength{\parindent}{1cm}

\usepackage{lastpage} % Узнать, сколько всего страниц в документе. 

\usepackage{soul} % Модификаторы начертания 

\usepackage{hyperref} 
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor} 
\hypersetup{ % Гиперссылки 
	unicode=true, % русские буквы в раздела PDF 
	pdftitle={Заголовок}, % Заголовок 
	pdfauthor={Автор}, % Автор 
	pdfsubject={Тема}, % Тема 
	pdfcreator={Создатель}, % Создатель 
	pdfproducer={Производитель}, % Производитель 
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова 
	colorlinks=true, % false: ссылки в рамках; true: цветные ссылки 
	linkcolor=red, % внутренние ссылки 
	citecolor=black, % на библиографию 
	filecolor=magenta, % на файлы
	urlcolor=blue % на URL 
} 

\usepackage{csquotes} % Еще инструменты для ссылок 

%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex} 

\usepackage{multicol} % Несколько колонок 

\usepackage{tikz} % Работа с графикой 
\usepackage{pgfplots} 
\usepackage{pgfplotstable} 
\usepackage{bbm}


\author
{Ilya Zharikov, Roman Isachenko, Artem Bochkarev} 
\title 
{Metamodels for complex structured objects classification}
\date{}

\begin{document} 
\maketitle

\section*{Introduction}
This project is dedicated to multiclass classification of complex structured objects (for which we don't have explicit features). 
The problem arises in many applications such as image recognition, signal processing or time series classification. 
We will focus on multiclass multivariate time series classification. 
In this setup time series are regarded as complex structured objects without explicit feature description. 
This is reasonable because we can't operate with original features as time series might be of different size, not aligned \cite{geurts2001pattern} or even multiscaled.

We investigate classification of accelerometer time series \cite{wang2014human}. The data is time series of acceleration from three axis, which is sensed by mobile phone or other portable device with accelerometer. 
The task is to predict the activity a person is performing. 
List of activities might include walking, running, sitting or walking up/down the stairs.

In general the problem of classifying complex structured objects can be split in two distinctive procedures. 
First, we need to extract informative features, and then we use those features as input to some classifier to obtain final model. 
For simplicity, we assume that these two procedures can be built and analyzed separately. 
In our project we focused mainly on comparing different methods of feature generation \cite{karasikov24feature, ivkin}. 
Extracted features can be later used for building classifiers and feature selection algorithms.

The first approach for feature generation is calculating expertly defined functions of time series \cite{kwapisz2011activity}. 
These functions include average value, standard deviation, mean absolute deviation and distribution for each component. 
We consider this approach a baseline, as it is the simplest method we use.

We compare baseline with more sophisticated parametric feature generation methods. 
One of them is autoregressive model \cite{lukashin2003adaptive}. 
For each time series we build parametric model and use those parameters as features for classification. 
We also consider the model of singular spectrum analysis of time series \cite{hassani2007singular}. 
We use eigenvalues of trajectory matrix as features for building classifier.

In the first part of these report we give definitions and make problem statement. In the second part we describe all of the proposed approaches in more detail. 
Last, we make the experiment on real accelerometer datasets \cite{wisdm, usc}, compare all methods and give conclusions and recommendations for practical use.

\section*{Problem Statement}
Let $\mathcal{S}$ be a space of complex structured objects, $Y$ is a finite set of class labels. 
Denote by $\mathfrak{D} = \{(s_i, y_i)\}_{i=1}^m$~-- given sample, where $s_i \in \mathcal{S}$ and $y_i \in Y$.

We consider the problem of recovering the function $f: \mathcal{S} \rightarrow Y$
\[
	y = f(s).
\]
Let $L(f, \mathfrak{D})$ be an error function which expresses the classification error of the function $f$ over the sample $\mathfrak{D}$. 
Our goal is to determine function $f^*$ which minimizes the error
\begin{equation}
	f^* = \argmin_{f} L(f, \mathfrak{D}).
	\label{eq::general_classification_task}
\end{equation}

We assume that the target function $f^*$ belongs to the class of function compositions $f = g \circ h$, where
\begin{itemize}
	\item 
	$h: \mathcal{S} \rightarrow H$ is a map from the original space $\mathcal{S}$ to the feature space $H \subset \mathbb{R}^n$;
	\item 
	$g: H \times \Theta \rightarrow Y$ is a parametric map from the feature space $H$ to the space of class labels $Y$. 
	The function $g$ is parametrized by a vector parameter $\boldsymbol{\theta} \in \Theta$. 
\end{itemize}

The determining of the function $f^*$ is equivalent to determining the functions $h^*$ and  $g^*$.

In this project we consider the following ways of generating the feature space~$H$:
\begin{itemize}
	\item 
	expert functions based on prior knowledge of the original objects. 
	These functions can be expressed as a set of statistics $\{h_i\}_{i=1}^n$, where $h_i: \mathcal{S} \rightarrow \mathbb{R}$.
	Thus, the description $\mathbf{h}$ of the object $s$ is the value of these statistics on the object 
	\[
		\mathbf{h} = h(s) = (h_1(s), \dots, h_n(s)).
	\]
	\item 
	hypothesis of data generation. 
	In this case the features are the estimated parameters of the considered hypothesis. 
	Let $S(s, \mathbf{h}, \boldsymbol{\lambda})$ be the error function which specifies the hypothesis, e.g. one could define the function $S$ as negative log-likelihood function [to do link]. 
	The optimal parameters $\widehat{\mathbf{h}}$ for object $s$ is obtained by
	\begin{equation}
		\widehat{\mathbf{h}} = \argmin_{\mathbf{h}} S( s, \mathbf{h}, \bm{\lambda}).
		\label{eq::optimal_description}
	\end{equation}
	The parameter $\bm{\lambda}$ is external structural parameter for the function $S$.
	The equation~\eqref{eq::optimal_description} determines the feature map $h:\mathcal{S} \rightarrow H$.
\end{itemize}

Given appropriate feature space $H$ and feature map $h$ we transform our original sample $\mathfrak{D} = \{s_i, y_i\}_{i=1}^m$ with complex structured objects to the new sample $\mathfrak{D}_H = \{\mathbf{h}_i, y_i\}_{i=1}^m$, where $\mathbf{h}_i = h(s_i) \in H$. 
The function $g(\mathbf{h}, \bm{\theta})$ is defined by its parameters vector $\bm{\theta} \in \Theta$. 
The optimal parameters~$\widehat{\bm{\theta}}$ are given by
\[
	\widehat{\bm{\theta}} = \argmin_{\bm{\theta}} L(g(\cdot, \bm{\theta}), \mathfrak{D}_H),
\]
where $L(\cdot, \cdot)$ is an analogue of the function~\eqref{eq::general_classification_task}.

In our project we consider accelerometer time series as complex objects. Time series is represented in following way:
\[
	s = (x_1, \dots, x_T) \in \mathcal{S},
\]
where $T$ denotes the length of time series. Now let us expand on different approaches of feature generation.

\section*{Feature generation}
\subsection*{Expert functions}
Given a set of complex objects $\{s_i\}_{i=1}^m$ we can extract features in a non-parametric way with a set of expert functions $\{h_j\}_{j=1}^n$. For time series~{\color{red} ![link]}, these functions could be mean, standard deviations, mean absolute deviations and distributions of the acceleration. The main drawback of this approach is that we are restricted by our choice of expert functions and for some types of data these functions might be impossible to derive.

\subsection*{Autoregressive model}
In this method we assume autoregressive model~{\color{red} ![link]} of the order $n$ as a hypothesis for generation of time series $s$. Each component of the object $s$ is a linear combination of the previous $n$ components 
\begin{equation*}
	x_t = w_0 + \sum_{j=1}^{n} w_j x_{t-j} + \epsilon_t,
\end{equation*}
where $\epsilon_t$ is a random noise. Prediction of the autoregressive model is defined by
\begin{equation}
	\hat{x}_t = w_0 + \sum_{j=1}^{n} w_j x_{t-j}.
	\label{eq::autoregression_prediction}
\end{equation}
For this method $n$ is a structural parameter and $\bm{\lambda} = n$. 

Feature description $\mathbf{h}$ of the object $s$ is given by optimal parameters of autoregressive model $\widehat{\mathbf{w}} = \{\widehat{w}_j\}_{j=0}^n$ for time series $s_i$. The hypothesis error function~\eqref{eq::optimal_description} in this case is the squared error between the original object $s$ and its prediction of the model~\eqref{eq::autoregression_prediction}. 

\begin{equation}
	\mathbf{h} = \widehat{\mathbf{w}} = \argmin_{\mathbf{w} \in \mathbb{R}^{n+1}} S(s, \mathbf{w}, \bm{\lambda}) = \argmin_{\mathbf{w} \in \mathbb{R}^{n+1}} \left( \sum_{t=n+1}^{T} \|x_t - \hat{x}_t\|^2\right).
	\label{eq::autoregression_description}
\end{equation}
The problem~\eqref{eq::autoregression_description} could be easily converted to the linear regression problem. Hence, for each initial time series $s$ we have to solve linear regression problem.

\subsection*{Singular spectrum analysis}
Alternative hypothesis for generation of time series is SSA (Singular Spectrum Analysis) model. We construct trajectory matrix for each time series $s=(x_1, \dots x_T)$:
\[
	\mathbf{X} = 
	\begin{pmatrix}
	x_1 & x_2 & \dots & x_n \\
	x_2 & x_3 & \dots & x_{n+1} \\
	\dots & \dots & \dots & \dots \\
	x_{T-n+1} & x_{T-n+2} & \dots & x_T
	\end{pmatrix}.
\]
Here $n$, called the window width, is an external structural parameter.
Let find singular decomposition of matrix $\mathbf{X}^{\T} \mathbf{X}$:
\[
	\mathbf{X}^{\T} \mathbf{X} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^{\T},
\]
where $\mathbf{U}$ is a unitary matrix and $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$ whose entries $\lambda_i$ are eigenvalues of $\mathbf{X}^{\T} \mathbf{X}$. In this case we use the spectrum of the matrix $\mathbf{X}^{\T} \mathbf{X}$ as feature description $\mathbf{h}$ of the object $s$
\[
	\mathbf{h} = (\lambda_1, \dots, \lambda_n).
\]
\subsection*{Splines}
One could approximate the time series by splines~{\color{red} [link]}. Spline is defined by its parameters:
\begin{itemize}
	\item 
	$\{\xi_\ell\}_{\ell=1}^L$~--- the set of knots;
	\item 
	$\{\mathbf{w}_\ell\}_{\ell=1}^{L-1}$ ~--- parameters of the models are built on the interval $[\xi_\ell; \xi_{\ell + 1}]$. The dimension of each parameter vector $\mathbf{w}_{\ell}$ depends on the spline order.
\end{itemize}
The feature description of the time series could be assumed as union of these parameters.
\[
	\bm{h} = h(s) = (\xi_1, \dots, \xi_L, \mathbf{w}_1, \dots, \mathbf{w}_{L-1}).
\]

\section*{Classification}
In this section we will describe our approach to classification of time series using newly generated feature. We use three different classification models: logistic regression, SVM and random forests.

\subsection*{Quality measure}
We chose accuracy score as our main quality measure function. We did that because this approach was used in previous articles \cite{karasikov24feature, ivkin} and this measure is easy to understand. Accuracy score is a relation correctly labeled objects and the total amount of objects in dataset:

\begin{equation*}
	\textrm{accuracy}(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^{m} \mathbbm{1} (y_i = \hat{y}_i),
\end{equation*}

where $\hat{y}_i$ is a prediction of the classifier. We calculated this measure for binary case (when we predict each class in particular) and in multilabel case.

\subsection*{Multiclass classification}
As we had numerous labels in our datasets we had to choose one of the multiclass approaches to classification. We decided to use one-vs-rest classification as a simple, yet effective approach. The main idea is that we train binary classifiers for each class label and then, on the prediction step, we classify new object according to the most confident classifier.

\subsection*{Logistic regression}
The first approach to classification we played with was regularized logistic regression. We seek to minimize the following loss function:

\begin{equation*}
	L(\bm{\theta}) = \sum_{i=1}^{m} \log(1 + \exp(-(\bm{\theta}^T\mathbf{h}_i)y_i)) + \frac{\lambda}{2} \|\bm{\theta}\|^2
\end{equation*}

\begin{equation*}
	\bm{\theta}^* = \argmin_{\mathbb{\theta}} L(\bm{\theta}) 
\end{equation*}

We make a prediction for a new object in a following way:
\begin{equation*}
	\hat{y} = \sgn (\bm{\theta}^T \mathbf{h})
\end{equation*}

\subsection*{SVM}
We also used classical binary SVM. The problem in this case can be formulated in a following way:

\begin{align*}
	\bm{\theta^*}  = \begin{pmatrix}
	\mathbf{w}\\ b\\ \bm{\xi}
	\end{pmatrix}= \argmin_{\mathbf{w}^*, b^*, \bm{\xi}^*}  &\frac{1}{2} \|\mathbf{w}\|^2 + C\sum_{i=1}^{m} \xi_i\\
	\textrm{s.t.} \quad &y_i(\mathbf{w}^T \mathbf{h}_i + b) \geq 1 - \xi_i\\
	&\xi_i \geq 0, \quad 1 \leq i \leq m.
\end{align*}
 
For new objects we can make a prediction 

\begin{equation*}
\hat{y} = \sgn (\mathbf{w}^T \mathbf{h} + b)
\end{equation*}

\subsection*{Random Forest}
{\color{red} TO DO}

\subsection*{\color{red} TO DO section}
\begin{itemize}
	\item the function $h$ is vectorial (discuss notations);
	\item add explanation(pics) of autoregressive model;
	\item splines;
	\item add literature;
	\item fix the literature style.
\end{itemize}

\section*{Experiment}
In this paper we consider 2 different smart phone based datasets: WISDM~\cite{wisdm} and USC-HAD~\cite{usc}. 
Data from smart phone accelerometer consists of information about acceleration along each of three axis. 
Time difference between measurements equals 50 ms. 
Each time series belongs to one activity. 
The distributions of time series activities for each datasets are presented in Table~\ref{tbl::activities_distributions}. 
The length of each time series equals 200 which accounts 10 second. 
In the Figure~\ref{fig::ts_example} the example of the time series for one activity of the specific person is given.

\begin{table}[H]
	\centering
	\caption{Activities distributions}
	\subfloat[WISDM]{
		\begin{tabular}{r|l|rr}
				\hline
				&\textbf{Activity}   & \multicolumn{2}{l}{\textbf{\# objects}} \\
				\hline
				1&Standing            &229      &5.30  \% \\
				2&Walking             &1917     &44.36 \% \\
				3&Upstairs            &466      &10.78 \% \\
				4&Sitting             &277      &6.41  \% \\
				5&Jogging             &1075     &24.88 \% \\
				6&Downstairs          &357      &8.26  \% \\
				\hline
			&Total & \multicolumn{2}{l}{4321}  \\
			\hline
		\end{tabular}}
	\hspace{0.5cm}
	\subfloat[USC-HAD]{
		\begin{tabular}{r|l|rr}
			\hline
			&\textbf{Activity} & \multicolumn{2}{l}{\textbf{\# objects}} \\ \hline
			1&Standing            &1167     &8.57  \% \\
			2&Elevator-up         &764      &5.61  \% \\
			3&Walking-forward     &1874     &13.76 \% \\
			4&Sitting             &1294     &9.50  \% \\
			5&Walking-downstairs  &951      &6.98  \% \\
			6&Sleeping            &1860     &13.66 \% \\
			7&Elevator-down       &763      &5.60  \% \\
			8&Walking-upstairs    &1018     &7.47  \% \\
			9&Jumping             &495      &3.63  \% \\
			10&Walking-right       &1305     &9.58  \% \\
			11&Walking-left        &1280     &9.40  \% \\
			12&Running             &849      &6.23  \% \\
			\hline 
			&Total              & \multicolumn{2}{l}{13620}\\ 
			\hline
		\end{tabular}}
	\label{tbl::activities_distributions}
\end{table}

\begin{figure}[h]
	\centering
	\label{fig::ts_example}
	\includegraphics[width=1\linewidth]{Presentation/ts_example.png}
	\caption{Time series example}
\end{figure}

For each dataset we applied the feature generation approaches described above: expert functions, autoregressive model, SSA, splines. 
We used three different widely used classification model for each generated feature description: logistic regression, support vector machine and random forest. 
The external structural parameters for feature generation procedures,  such as the length $n$ for autoregression, the window width $n$ for SSA and the number of splines knots $L$, were tuned using 3-fold cross validation procedure. 
The hyperparameters for classification models were also tuned using the same cross validation procedure. 

In paper~\cite{kwapisz2011activity} the authors proposed to use the following expert functions for time series classification:
\begin{itemize}
	\item (3) average acceleration for each axis;
	\item (3) standard deviation for each axis;
	\item (3) average absolute difference for each axis;
	\item (1) average resultant acceleration;
	\item (30) values of histogram with 10 bins for each axis.
\end{itemize}
Autoregressive parameters + SSA {\color{red} TO DO}.

We fit splines for time series using $scipy$~{\color{red} [link]} python library. The spline order equals 3. The knots $\{\xi_{\ell}\}_{\ell = 1}^L$ for  splines were distributed uniformly. The number~$L$ were chosen implicitly by fitting smoothing parameter~$s$, it is possible because $L \sim \frac{1}{s}$. The fitting was constructed as follows. Firstly, there was the initialization step to find appropriate bounds for smoothing parameter. The next step is finding the smoothing parameter in this interval using bi-search approach.

The number of features for each method are
\begin{itemize}
	\item expert features: 40;
	\item autoregressive model: 63;
	\item singular spectrum analysis: 60;
	\item splines: 33.
\end{itemize}

The results of the experiments for the both datasets is presented in Figure~\ref{fig::accuracy_results}. For WISDM dataset the worst result is obtained by splines parameters. The results for expert functions, autoregressive model and SSA is roughly identical. For USC-HAD dataset the results is highly depend on the classification model. For both datasets logistic regression shows the worst quality, while the accuracy for support vector machine and random forest are strongly correlated.

\begin{figure}[h]
	\centering
	\subfloat{
	\includegraphics[width=0.49\linewidth]{presentation/wisdm_methods.png}}
	\subfloat{
	\includegraphics[width=0.49\linewidth]{presentation/uschad_methods.png}}
	\caption{Accuracy scores}
	\label{fig::accuracy_results}
\end{figure}

We also carried out the experiment for union of all $196$ generated features. The results are demonstrated on the Figure~\ref{fig::feature_union_results}. In the Table~\ref{tbl::activities_distributions} one can see class labels, that are represented on the corresponding histograms.

\begin{figure}[h]
	\centering
	\subfloat[WISDM dataset]{
		\includegraphics[width=0.33\linewidth]{pics/hist_wisdm_lr_all.png}}
	\subfloat[USC-HAD dataset]{
		\includegraphics[width=0.66\linewidth]{pics/hist_uschad_lr_all.png}}\\
	\subfloat[WISDM dataset]{
		\includegraphics[width=0.33\linewidth]{pics/hist_wisdm_rf_all.png}}
	\subfloat[USC-HAD dataset]{
		\includegraphics[width=0.66\linewidth]{pics/hist_uschad_rf_all.png}}\\
	\subfloat[WISDM dataset]{
		\includegraphics[width=0.33\linewidth]{pics/hist_wisdm_svm_all.png}}
	\subfloat[USC-HAD dataset]{
		\includegraphics[width=0.66\linewidth]{pics/hist_uschad_svm_all.png}}\\
	\caption{Accuracy scores of classification of each class using all features}
	\label{fig::feature_union_results}
\end{figure}

\section*{Conclusion}

The problem of complex structured objects classification were considered. 
We investigated the different approaches of feature extraction, particularly the expert functions and data generation hypothesis. 
The experiment on the real data from smart phone accelerometer were carried out. 
We compared different feature descriptions and different classification models. 
The results show that obtained features allows to recover the class label with the high quality.

\bibliographystyle{unsrt}
\bibliography{ref.bib}
\end{document}